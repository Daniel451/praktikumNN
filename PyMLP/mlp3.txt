
============================================================================

a: [[0 1]]
self.W[l]: [[-0.133301604   0.0008572114]
 [ 0.0777116726 -0.3206972183]
 [-0.0198465845 -0.4098421422]]
self.B[l]: [[ 0.0848112323]
 [ 0.4111778799]
 [ 0.0436669983]]
a: [[ 0.0856684437  0.0904806616 -0.3661751439]]
self.W[l]: [[-0.2522253992  0.093474046  -0.1166710151]]
self.B[l]: [[ 0.1515373456]]
Gewichte vor lernen:
[array([[-0.133301604 ,  0.0008572114],
       [ 0.0777116726, -0.3206972183],
       [-0.0198465845, -0.4098421422]]), array([[-0.2522253992,  0.093474046 , -0.1166710151]])]
Bias vor lernen:
[array([[ 0.0848112323],
       [ 0.4111778799],
       [ 0.0436669983]]), array([[ 0.1515373456]])]
Delta des Inputs: [ 0.8188907925]
+++++++++++++++++++++++++
Passe Layer an: 1
mit delta:    [ 0.8188907925]
mit a:        [[ 0.1811092075]]
Output[l] : [[ 0.0856684437  0.0904806616 -0.3661751439]]
_tanh_deriv(Output[l]) : [[ 0.9926966771  0.9918577254  0.8770504473]]
delta next:[[-0.1480084236  0.1361024695 -0.2991543018]]
+++++++++++++++++++++++++
Output ist: [array([[0, 1]]), array([[ 0.0856684437,  0.0904806616, -0.3661751439]]), array([[ 0.1811092075]])]
ENDE!!!!
Gewichte nach lernen:
[array([[-0.133301604 ,  0.0008572114],
       [ 0.0777116726, -0.3206972183],
       [-0.0198465845, -0.4098421422]]), array([[-0.1820722994,  0.1675678267, -0.4165284689]])]
Bias nach lernen:
[array([[ 0.0848112323],
       [ 0.4111778799],
       [ 0.0436669983]]), array([[ 0.9704281381]])]
[0, 0] [[ 0.7624785935]]
[0, 1] [[ 0.8061874254]]
[1, 0] [[ 0.7799701293]]
[1, 1] [[ 0.8211267334]]
